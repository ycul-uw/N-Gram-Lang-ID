{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Lucy Wu*\n",
    "\n",
    "*Date: 03/12/2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram LETTER Language Identification Model\n",
    "\n",
    "This notebook implements a bigram LETTER language identification model. The model is trained on text data from three languages: English, French, and Italian. The goal is to classify sentences from a validation set into one of these languages. The models are trained both with and without Add-One smoothing to handle unseen bigrams.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Preprocessing Function**:\n",
    "    - `preprocess(text)`: Converts text to lowercase and removes spaces and newlines.\n",
    "\n",
    "2. **Training Bigram Model**:\n",
    "    - `train_bigram_model(file_path)`: Trains a bigram model for a given language by counting occurrences of bigrams and unigrams, and then converting these counts to probabilities using Add-One smoothing.\n",
    "\n",
    "3. **Train Models for Each Language**:\n",
    "    - We train bigram models for English, French, and Italian using the respective training files.\n",
    "\n",
    "4. **Compute Sentence Probability**:\n",
    "    - `compute_sentence_probability(sentence, bigram_model)`: Computes the log probability of a sentence under a given bigram model.\n",
    "\n",
    "5. **Classify Validation Data**:\n",
    "    - We read sentences from the validation file and compute their probabilities under each language model.\n",
    "    - The language with the highest probability is chosen as the predicted language for each sentence.\n",
    "\n",
    "6. **Output Results**:\n",
    "    - The classification results are written to an output file.\n",
    "\n",
    "7. **Compare Output with Solution**:\n",
    "    - `compare_files(output_file, solution_file)`: Compares the output file with a solution file to identify differences.\n",
    "\n",
    "### Variables\n",
    "\n",
    "- `TRAINING_FILES`: Dictionary mapping each language to its corresponding training file.\n",
    "- `TRAINING_PATH`: Path to the directory containing training data.\n",
    "- `VALIDATION_FILE`: Path to the validation file.\n",
    "- `LANGUAGES`: List of languages being considered.\n",
    "- `bigram_models`: Dictionary containing trained bigram models for each language.\n",
    "- `OUTPUT_FILE`: Path to the output file where classification results are stored.\n",
    "\n",
    "This model uses bigram probabilities to classify sentences into one of the three languages, providing a simple yet effective approach to language identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths for training data and validation file\n",
    "TRAINING_PATH = \"../Data/Input/\"\n",
    "VALIDATION_FILE = \"../Data/Validation/LangId.test\"\n",
    "\n",
    "# Dictionary containing file names for each language\n",
    "TRAINING_FILES = {\n",
    "    \"English\": \"LangId.train.English\",\n",
    "    \"French\": \"LangId.train.French\",\n",
    "    \"Italian\": \"LangId.train.Italian\"\n",
    "}\n",
    "\n",
    "# List of languages\n",
    "LANGUAGES = [\"English\", \"French\", \"Italian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text by converting to lowercase and removing spaces/newlines\n",
    "def preprocess(text):\n",
    "    return text.lower().replace(\"\\n\", \" \").replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Training Models (without smoothing)\n",
    "This version of models only calculate probabilities based on observed counts without adjusting for unseen LETTER bigrams.\n",
    "\n",
    "The models assign negative infinity to unseen bigrams instead of smoothing, meaning they will strictly rely on observed data.\n",
    "\n",
    "***Skip the next block and jump to `Classify Validation Data` section to see different output results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a bigram model from a given training file\n",
    "def train_bigram_model(file_path):\n",
    "    # Dictionaries to store bigram counts and unigram counts\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    \n",
    "    # Open the training file and preprocess the text\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = preprocess(f.read())\n",
    "        \n",
    "        # Count occurrences of bigrams and unigrams\n",
    "        for i in range(len(text) - 1):\n",
    "            bigram = (text[i], text[i + 1])\n",
    "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "            unigram_counts[bigram[0]] += 1\n",
    "    \n",
    "    # Convert bigram counts to probabilities (without smoothing)\n",
    "    bigram_probs = {}\n",
    "    for first_char, following_chars in bigram_counts.items():\n",
    "        total = unigram_counts[first_char]  # No smoothing\n",
    "        bigram_probs[first_char] = {char: count / total for char, count in following_chars.items()}\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "# Train bigram models for each language\n",
    "bigram_models = {}\n",
    "for lang, filename in TRAINING_FILES.items():\n",
    "    file_path = os.path.join(TRAINING_PATH, filename)\n",
    "    bigram_models[lang] = train_bigram_model(file_path)\n",
    "\n",
    "# Function to compute log probability of a sentence under a given bigram model\n",
    "def compute_sentence_probability(sentence, bigram_model):\n",
    "    sentence = preprocess(sentence)  # Preprocess the sentence\n",
    "    log_prob = 0  # Initialize log probability to 0\n",
    "    \n",
    "    # Iterate over character bigrams in the sentence\n",
    "    for i in range(len(sentence) - 1):\n",
    "        first, second = sentence[i], sentence[i + 1]\n",
    "        \n",
    "        # Retrieve bigram probability, defaulting to a very low probability if not found\n",
    "        prob = bigram_model.get(first, {}).get(second, 0)\n",
    "        \n",
    "        if prob > 0:\n",
    "            log_prob += math.log(prob)\n",
    "        else:\n",
    "            return float('-inf')  # Assign negative infinity if an unseen bigram is encountered\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Training Models (with Add-One smoothing)\n",
    "A different implementation than the previous block by adding Add-One smoothing to handle unseen LETTER bigrams.\n",
    "\n",
    "***Re-run the `Classify Validation Data` section to see different output results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a bigram model from a given training file\n",
    "def train_bigram_model(file_path):\n",
    "    # Dictionaries to store bigram counts and unigram counts\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    \n",
    "    # Open the training file and preprocess the text\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = preprocess(f.read())\n",
    "        \n",
    "        # Count occurrences of bigrams and unigrams\n",
    "        for i in range(len(text) - 1):\n",
    "            bigram = (text[i], text[i + 1])\n",
    "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "            unigram_counts[bigram[0]] += 1\n",
    "    \n",
    "    # Convert bigram counts to probabilities using Add-One smoothing\n",
    "    bigram_probs = {}\n",
    "    for first_char, following_chars in bigram_counts.items():\n",
    "        total = unigram_counts[first_char] + len(bigram_counts)  # Add vocabulary size for smoothing\n",
    "        bigram_probs[first_char] = {char: (count + 1) / total for char, count in following_chars.items()}\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "# Train bigram models for each language\n",
    "bigram_models = {}\n",
    "for lang, filename in TRAINING_FILES.items():\n",
    "    file_path = os.path.join(TRAINING_PATH, filename)\n",
    "    bigram_models[lang] = train_bigram_model(file_path)\n",
    "\n",
    "# Function to compute log probability of a sentence under a given bigram model\n",
    "def compute_sentence_probability(sentence, bigram_model):\n",
    "    sentence = preprocess(sentence)  # Preprocess the sentence\n",
    "    log_prob = 0  # Initialize log probability to 0\n",
    "    \n",
    "    # Iterate over character bigrams in the sentence\n",
    "    for i in range(len(sentence) - 1):\n",
    "        first, second = sentence[i], sentence[i + 1]\n",
    "        \n",
    "        # Retrieve bigram probability; apply add-one smoothing if necessary\n",
    "        prob = bigram_model.get(first, {}).get(second, 1 / (sum(bigram_model.get(first, {}).values()) + len(bigram_model)))\n",
    "        \n",
    "        # Add the log probability (log avoids numerical underflow)\n",
    "        log_prob += math.log(prob)\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and classify validation data\n",
    "with open(VALIDATION_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_sentences = f.readlines()  # Read test sentences line by line\n",
    "\n",
    "results = []\n",
    "\n",
    "# Compute probabilities for each sentence in the test set\n",
    "for idx, sentence in enumerate(test_sentences, start=1):\n",
    "    # Calculate the probability of the sentence under each language model\n",
    "    scores = {lang: compute_sentence_probability(sentence, bigram_models[lang]) for lang in LANGUAGES}\n",
    "    \n",
    "    # Determine the language with the highest probability\n",
    "    predicted_language = max(scores, key=scores.get)\n",
    "    \n",
    "    # Store the result as \"[line_number] [predicted_language]\"\n",
    "    results.append(f\"{idx} {predicted_language}\")\n",
    "\n",
    "# Write the classification results\n",
    "OUTPUT_FILE = \"../Data/Output/letterLangId.out\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Output with Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 22 is different:\n",
      "Output: 22 Italian\n",
      "Solution: 22 French\n",
      "\n",
      "Line 24 is different:\n",
      "Output: 24 English\n",
      "Solution: 24 French\n",
      "\n",
      "Line 25 is different:\n",
      "Output: 25 English\n",
      "Solution: 25 Italian\n",
      "\n",
      "Line 43 is different:\n",
      "Output: 43 English\n",
      "Solution: 43 Italian\n",
      "\n",
      "Line 44 is different:\n",
      "Output: 44 English\n",
      "Solution: 44 Italian\n",
      "\n",
      "Line 60 is different:\n",
      "Output: 60 English\n",
      "Solution: 60 French\n",
      "\n",
      "Line 66 is different:\n",
      "Output: 66 English\n",
      "Solution: 66 French\n",
      "\n",
      "Line 83 is different:\n",
      "Output: 83 English\n",
      "Solution: 83 Italian\n",
      "\n",
      "Line 91 is different:\n",
      "Output: 91 English\n",
      "Solution: 91 Italian\n",
      "\n",
      "Line 111 is different:\n",
      "Output: 111 English\n",
      "Solution: 111 French\n",
      "\n",
      "Line 169 is different:\n",
      "Output: 169 English\n",
      "Solution: 169 Italian\n",
      "\n",
      "Line 185 is different:\n",
      "Output: 185 English\n",
      "Solution: 185 French\n",
      "\n",
      "Line 190 is different:\n",
      "Output: 190 English\n",
      "Solution: 190 Italian\n",
      "\n",
      "Line 221 is different:\n",
      "Output: 221 English\n",
      "Solution: 221 French\n",
      "\n",
      "Line 223 is different:\n",
      "Output: 223 English\n",
      "Solution: 223 Italian\n",
      "\n",
      "Line 232 is different:\n",
      "Output: 232 French\n",
      "Solution: 232 Italian\n",
      "\n",
      "Line 242 is different:\n",
      "Output: 242 English\n",
      "Solution: 242 Italian\n",
      "\n",
      "Line 252 is different:\n",
      "Output: 252 English\n",
      "Solution: 252 French\n",
      "\n",
      "Line 276 is different:\n",
      "Output: 276 English\n",
      "Solution: 276 French\n",
      "\n",
      "Line 279 is different:\n",
      "Output: 279 English\n",
      "Solution: 279 French\n",
      "\n",
      "Line 280 is different:\n",
      "Output: 280 English\n",
      "Solution: 280 French\n",
      "\n",
      "Line 284 is different:\n",
      "Output: 284 English\n",
      "Solution: 284 Italian\n",
      "\n",
      "Line 298 is different:\n",
      "Output: 298 English\n",
      "Solution: 298 Italian\n",
      "\n",
      "Line 299 is different:\n",
      "Output: 299 English\n",
      "Solution: 299 French\n",
      "\n",
      "Total number of wrong predictions: 24\n",
      "Model Accuracy: 92.0%\n"
     ]
    }
   ],
   "source": [
    "def compare_files(output_file, solution_file):\n",
    "    with open(output_file, 'r') as f1, open(solution_file, 'r') as f2:\n",
    "        output_lines = f1.readlines()\n",
    "        solution_lines = f2.readlines()\n",
    "    \n",
    "    # Ensure both files have the same number of lines\n",
    "    if (len(output_lines) != len(solution_lines)):\n",
    "        print(\"Files have different number of lines.\")\n",
    "\n",
    "    # Counter for the number of differences\n",
    "    diff_count = 0\n",
    "\n",
    "    for i in range(len(output_lines)):\n",
    "        if output_lines[i] != solution_lines[i]:\n",
    "            diff_count += 1\n",
    "            print(f\"Line {i + 1} is different:\")\n",
    "            print(f\"Output: {output_lines[i].strip()}\")\n",
    "            print(f\"Solution: {solution_lines[i].strip()}\")\n",
    "            print()\n",
    "\n",
    "    # Print the total number of differences\n",
    "    print(f\"Total number of wrong predictions: {diff_count}\")\n",
    "    print(f\"Model Accuracy: {(300-diff_count)/3}%\")\n",
    "    \n",
    "# File paths\n",
    "output_file = '../Data/Output/letterLangId.out'\n",
    "solution_file = '../Data/Validation/labels.sol'\n",
    "\n",
    "# Call the function\n",
    "compare_files(output_file, solution_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
