{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Lucy Wu*\n",
    "\n",
    "*Date: 03/12/2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram WORD Language Identification Model 1\n",
    "\n",
    "This notebook implements a bigram WORD language identification model. The model is trained on text data from three languages: English, French, and Italian. The goal is to classify sentences from a validation set into one of these languages. The models are trained both with and without Add-One smoothing to handle unseen bigrams.\n",
    "\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Preprocessing Function**:\n",
    "    - `preprocess(text)`: Converts text to lowercase but spaces are kept.\n",
    "\n",
    "2. **Training Bigram Model**:\n",
    "    - `train_bigram_model(file_path)`: Trains a bigram model for a given language by counting occurrences of bigrams and unigrams, and then converting these counts to probabilities using Add-One smoothing.\n",
    "\n",
    "3. **Train Models for Each Language**:\n",
    "    - We train bigram models for English, French, and Italian using the respective training files.\n",
    "\n",
    "4. **Compute Sentence Probability**:\n",
    "    - `compute_sentence_probability(sentence, bigram_model)`: Computes the log probability of a sentence under a given bigram model.\n",
    "\n",
    "5. **Classify Validation Data**:\n",
    "    - We read sentences from the validation file and compute their probabilities under each language model.\n",
    "    - The language with the highest probability is chosen as the predicted language for each sentence.\n",
    "\n",
    "6. **Output Results**:\n",
    "    - The classification results are written to an output file.\n",
    "\n",
    "7. **Compare Output with Solution**:\n",
    "    - `compare_files(output_file, solution_file)`: Compares the output file with a solution file to identify differences.\n",
    "\n",
    "### Variables\n",
    "\n",
    "- `TRAINING_FILES`: Dictionary mapping each language to its corresponding training file.\n",
    "- `TRAINING_PATH`: Path to the directory containing training data.\n",
    "- `VALIDATION_FILE`: Path to the validation file.\n",
    "- `LANGUAGES`: List of languages being considered.\n",
    "- `bigram_models`: Dictionary containing trained bigram models for each language.\n",
    "- `OUTPUT_FILE`: Path to the output file where classification results are stored.\n",
    "\n",
    "This model uses bigram probabilities to classify sentences into one of the three languages, providing a simple yet effective approach to language identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths for training data and validation file\n",
    "TRAINING_PATH = \"../Data/Input/\"\n",
    "VALIDATION_FILE = \"../Data/Validation/LangId.test\"\n",
    "\n",
    "# Dictionary containing file names for each language\n",
    "TRAINING_FILES = {\n",
    "    \"English\": \"LangId.train.English\",\n",
    "    \"French\": \"LangId.train.French\",\n",
    "    \"Italian\": \"LangId.train.Italian\"\n",
    "}\n",
    "\n",
    "# List of languages\n",
    "LANGUAGES = [\"English\", \"French\", \"Italian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text by converting to lowercase and keeping spaces\n",
    "def preprocess(text):\n",
    "    return text.lower().replace(\"\\n\", \" \").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Training Models (without smoothing)\n",
    "This version of models only calculate probabilities based on observed counts without adjusting for unseen WORD bigrams.\n",
    "\n",
    "The models assign negative infinity to unseen bigrams instead of smoothing, meaning they will strictly rely on observed data.\n",
    "\n",
    "***Skip the next block and jump to `Classify Validation Data` section to see different output results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a bigram model from a given training file\n",
    "def train_bigram_model(file_path):\n",
    "    # Dictionaries to store bigram counts and unigram counts\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    \n",
    "    # Open the training file and preprocess the text\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = preprocess(f.read())\n",
    "        \n",
    "        # Count occurrences of bigrams and unigrams\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i + 1])\n",
    "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "            unigram_counts[bigram[0]] += 1\n",
    "    \n",
    "    # Convert bigram counts to probabilities (without smoothing)\n",
    "    bigram_probs = {}\n",
    "    for first_word, following_words in bigram_counts.items():\n",
    "        total = unigram_counts[first_word]  # No smoothing\n",
    "        bigram_probs[first_word] = {word: count / total for word, count in following_words.items()}\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "# Train bigram models for each language\n",
    "bigram_models = {}\n",
    "for lang, filename in TRAINING_FILES.items():\n",
    "    file_path = os.path.join(TRAINING_PATH, filename)\n",
    "    bigram_models[lang] = train_bigram_model(file_path)\n",
    "\n",
    "# Function to compute log probability of a sentence under a given bigram model\n",
    "def compute_sentence_probability(sentence, bigram_model):\n",
    "    words = preprocess(sentence)  # Preprocess the sentence\n",
    "    log_prob = 0  # Initialize log probability to 0\n",
    "    \n",
    "    # Iterate over word bigrams in the sentence\n",
    "    for i in range(len(words) - 1):\n",
    "        first, second = words[i], words[i + 1]\n",
    "        \n",
    "        # Retrieve bigram probability, defaulting to a very low probability if not found\n",
    "        prob = bigram_model.get(first, {}).get(second, 0)\n",
    "        \n",
    "        if prob > 0:\n",
    "            log_prob += math.log(prob)\n",
    "        else:\n",
    "            return float('-inf')  # Assign negative infinity if an unseen bigram is encountered\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Training Models (with Add-One smoothing)\n",
    "A different implementation than the previous block by adding Add-One smoothing to handle unseen WORD bigrams.\n",
    "\n",
    "***Re-run the `Classify Validation Data` section to see different output results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a bigram model from a given training file\n",
    "def train_bigram_model(file_path):\n",
    "    # Dictionaries to store bigram counts and unigram counts\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    vocabulary = set()\n",
    "    \n",
    "    # Open the training file and preprocess the text\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = preprocess(f.read())\n",
    "        vocabulary.update(words)\n",
    "        \n",
    "        # Count occurrences of bigrams and unigrams\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i + 1])\n",
    "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "            unigram_counts[bigram[0]] += 1\n",
    "    \n",
    "    # Convert bigram counts to probabilities using Add-One Smoothing\n",
    "    bigram_probs = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    for first_word, following_words in bigram_counts.items():\n",
    "        total = unigram_counts[first_word] + vocab_size  # Add vocabulary size for smoothing\n",
    "        bigram_probs[first_word] = {word: (count + 1) / total for word, count in following_words.items()}\n",
    "        \n",
    "        # Ensure all vocabulary words have at least a small probability\n",
    "        for word in vocabulary:\n",
    "            if word not in bigram_probs[first_word]:\n",
    "                bigram_probs[first_word][word] = 1 / total\n",
    "    \n",
    "    return bigram_probs\n",
    "\n",
    "# Train bigram models for each language\n",
    "bigram_models = {}\n",
    "for lang, filename in TRAINING_FILES.items():\n",
    "    file_path = os.path.join(TRAINING_PATH, filename)\n",
    "    bigram_models[lang] = train_bigram_model(file_path)\n",
    "\n",
    "# Function to compute log probability of a sentence under a given bigram model\n",
    "def compute_sentence_probability(sentence, bigram_model):\n",
    "    words = preprocess(sentence)  # Preprocess the sentence\n",
    "    log_prob = 0  # Initialize log probability to 0\n",
    "    \n",
    "    # Iterate over word bigrams in the sentence\n",
    "    for i in range(len(words) - 1):\n",
    "        first, second = words[i], words[i + 1]\n",
    "        \n",
    "        # Retrieve bigram probability with add-one smoothing\n",
    "        prob = bigram_model.get(first, {}).get(second, 1 / (sum(bigram_model.get(first, {}).values()) + len(bigram_model)))\n",
    "        \n",
    "        log_prob += math.log(prob)\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and classify validation data\n",
    "with open(VALIDATION_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_sentences = f.readlines()  # Read test sentences line by line\n",
    "\n",
    "results = []\n",
    "\n",
    "# Compute probabilities for each sentence in the test set\n",
    "for idx, sentence in enumerate(test_sentences, start=1):\n",
    "    # Calculate the probability of the sentence under each language model\n",
    "    scores = {lang: compute_sentence_probability(sentence, bigram_models[lang]) for lang in LANGUAGES}\n",
    "    \n",
    "    # Determine the language with the highest probability\n",
    "    predicted_language = max(scores, key=scores.get)\n",
    "    \n",
    "    # Store the result as \"[line_number] [predicted_language]\"\n",
    "    results.append(f\"{idx} {predicted_language}\")\n",
    "\n",
    "# Write the classification results\n",
    "OUTPUT_FILE = \"../Data/Output/wordLangId.out\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Output with Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 44 is different:\n",
      "Output: 44 English\n",
      "Solution: 44 Italian\n",
      "\n",
      "Line 244 is different:\n",
      "Output: 244 English\n",
      "Solution: 244 Italian\n",
      "\n",
      "Line 262 is different:\n",
      "Output: 262 English\n",
      "Solution: 262 Italian\n",
      "\n",
      "Line 297 is different:\n",
      "Output: 297 English\n",
      "Solution: 297 Italian\n",
      "\n",
      "Total number of wrong predictions: 4\n",
      "Model Accuracy: 98.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "def compare_files(output_file, solution_file):\n",
    "    with open(output_file, 'r') as f1, open(solution_file, 'r') as f2:\n",
    "        output_lines = f1.readlines()\n",
    "        solution_lines = f2.readlines()\n",
    "    \n",
    "    # Ensure both files have the same number of lines\n",
    "    if (len(output_lines) != len(solution_lines)):\n",
    "        print(\"Files have different number of lines.\")\n",
    "\n",
    "    # Counter for the number of differences\n",
    "    diff_count = 0\n",
    "\n",
    "    for i in range(len(output_lines)):\n",
    "        if output_lines[i] != solution_lines[i]:\n",
    "            diff_count += 1\n",
    "            print(f\"Line {i + 1} is different:\")\n",
    "            print(f\"Output: {output_lines[i].strip()}\")\n",
    "            print(f\"Solution: {solution_lines[i].strip()}\")\n",
    "            print()\n",
    "\n",
    "    # Print the total number of differences\n",
    "    print(f\"Total number of wrong predictions: {diff_count}\")\n",
    "    print(f\"Model Accuracy: {(300-diff_count)/3}%\")\n",
    "    \n",
    "# File paths\n",
    "output_file = '../Data/Output/wordLangId.out'\n",
    "solution_file = '../Data/Validation/labels.sol'\n",
    "\n",
    "# Call the function\n",
    "compare_files(output_file, solution_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
