{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Lucy Wu*\n",
    "\n",
    "*Date: 03/12/2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram WORD Language Identification Model 2\n",
    "\n",
    "This notebook implements a bigram WORD language identification model. The model is trained on text data from three languages: English, French, and Italian. The goal is to classify sentences from a validation set into one of these languages. The models are trained with Good-Turing smoothing specifically.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Preprocessing Function**:\n",
    "    - `preprocess(text)`: Converts text to lowercase but spaces are kept.\n",
    "\n",
    "2. **Training Bigram Model**:\n",
    "    - `train_bigram_model(file_path)`: Trains a bigram model for a given language by counting occurrences of bigrams and unigrams, and then converting these counts to probabilities using Good-Turing smoothing.\n",
    "\n",
    "3. **Train Models for Each Language**:\n",
    "    - We train bigram models for English, French, and Italian using the respective training files.\n",
    "\n",
    "4. **Compute Sentence Probability**:\n",
    "    - `compute_sentence_probability(sentence, bigram_model)`: Computes the log probability of a sentence under a given bigram model.\n",
    "\n",
    "5. **Classify Validation Data**:\n",
    "    - We read sentences from the validation file and compute their probabilities under each language model.\n",
    "    - The language with the highest probability is chosen as the predicted language for each sentence.\n",
    "\n",
    "6. **Output Results**:\n",
    "    - The classification results are written to an output file.\n",
    "\n",
    "7. **Compare Output with Solution**:\n",
    "    - `compare_files(output_file, solution_file)`: Compares the output file with a solution file to identify differences.\n",
    "\n",
    "### Variables\n",
    "\n",
    "- `TRAINING_FILES`: Dictionary mapping each language to its corresponding training file.\n",
    "- `TRAINING_PATH`: Path to the directory containing training data.\n",
    "- `VALIDATION_FILE`: Path to the validation file.\n",
    "- `LANGUAGES`: List of languages being considered.\n",
    "- `bigram_models`: Dictionary containing trained bigram models for each language.\n",
    "- `OUTPUT_FILE`: Path to the output file where classification results are stored.\n",
    "\n",
    "This model uses bigram probabilities to classify sentences into one of the three languages, providing a simple yet effective approach to language identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths for training data and validation file\n",
    "TRAINING_PATH = \"../Data/Input/\"\n",
    "VALIDATION_FILE = \"../Data/Validation/LangId.test\"\n",
    "\n",
    "# Dictionary containing file names for each language\n",
    "TRAINING_FILES = {\n",
    "    \"English\": \"LangId.train.English\",\n",
    "    \"French\": \"LangId.train.French\",\n",
    "    \"Italian\": \"LangId.train.Italian\"\n",
    "}\n",
    "\n",
    "# List of languages\n",
    "LANGUAGES = [\"English\", \"French\", \"Italian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text by converting to lowercase and keeping spaces\n",
    "def preprocess(text):\n",
    "    return text.lower().replace(\"\\n\", \" \").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Training Models (with Add-One smoothing)\n",
    "A different implementation than the previous block by adding Add-One smoothing to handle unseen WORD bigrams.\n",
    "\n",
    "***Re-run the `Classify Validation Data` section to see different output results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a bigram model with Good-Turing smoothing\n",
    "def train_bigram_model(file_path):\n",
    "    # Dictionaries to store bigram and unigram counts\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    frequency_of_counts = defaultdict(int)  # Tracks how many bigrams have each count\n",
    "    vocabulary = set()\n",
    "\n",
    "    # Open and preprocess the training file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = preprocess(f.read())\n",
    "        vocabulary.update(words)\n",
    "        \n",
    "        # Count occurrences of bigrams and unigrams\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i + 1])\n",
    "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "            unigram_counts[bigram[0]] += 1\n",
    "\n",
    "    # Compute frequency of counts for Good-Turing smoothing\n",
    "    for first_word, following_words in bigram_counts.items():\n",
    "        for second_word, count in following_words.items():\n",
    "            frequency_of_counts[count] += 1\n",
    "    \n",
    "    # Compute Good-Turing smoothed probabilities\n",
    "    bigram_probs = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    # Total number of bigrams\n",
    "    total_bigrams = sum(sum(following_words.values()) for following_words in bigram_counts.values())\n",
    "\n",
    "    for first_word, following_words in bigram_counts.items():\n",
    "        bigram_probs[first_word] = {}\n",
    "        \n",
    "        for second_word, count in following_words.items():\n",
    "            # Apply Good-Turing estimation\n",
    "            if count + 1 in frequency_of_counts and frequency_of_counts[count] > 0:\n",
    "                adjusted_count = (count + 1) * (frequency_of_counts[count + 1] / frequency_of_counts[count])\n",
    "            else:\n",
    "                adjusted_count = count  # Default to normal count if no higher count exists\n",
    "\n",
    "            bigram_probs[first_word][second_word] = adjusted_count / total_bigrams\n",
    "\n",
    "        # Assign probability for unseen bigrams using Good-Turing smoothing\n",
    "        unseen_prob = frequency_of_counts[1] / total_bigrams if frequency_of_counts[1] > 0 else 1 / total_bigrams\n",
    "        for word in vocabulary:\n",
    "            if word not in bigram_probs[first_word]:\n",
    "                bigram_probs[first_word][word] = unseen_prob\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "# Train bigram models for each language\n",
    "bigram_models = {}\n",
    "for lang, filename in TRAINING_FILES.items():\n",
    "    file_path = os.path.join(TRAINING_PATH, filename)\n",
    "    bigram_models[lang] = train_bigram_model(file_path)\n",
    "\n",
    "# Function to compute log probability of a sentence under a given bigram model\n",
    "def compute_sentence_probability(sentence, bigram_model):\n",
    "    words = preprocess(sentence)  # Preprocess the sentence\n",
    "    log_prob = 0  # Initialize log probability\n",
    "    \n",
    "    # Iterate over word bigrams in the sentence\n",
    "    for i in range(len(words) - 1):\n",
    "        first, second = words[i], words[i + 1]\n",
    "\n",
    "        # Retrieve bigram probability safely\n",
    "        first_word_probs = bigram_model.get(first, {})\n",
    "        total_prob_mass = sum(first_word_probs.values())\n",
    "\n",
    "        if total_prob_mass > 0:\n",
    "            prob = first_word_probs.get(second, first_word_probs.get(\"__unseen__\", 1 / total_prob_mass))\n",
    "        else:\n",
    "            prob = 1e-10  # Small fallback probability for unseen cases\n",
    "\n",
    "        log_prob += math.log(prob)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and classify validation data\n",
    "with open(VALIDATION_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_sentences = f.readlines()  # Read test sentences line by line\n",
    "\n",
    "results = []\n",
    "\n",
    "# Compute probabilities for each sentence in the test set\n",
    "for idx, sentence in enumerate(test_sentences, start=1):\n",
    "    # Calculate the probability of the sentence under each language model\n",
    "    scores = {lang: compute_sentence_probability(sentence, bigram_models[lang]) for lang in LANGUAGES}\n",
    "    \n",
    "    # Determine the language with the highest probability\n",
    "    predicted_language = max(scores, key=scores.get)\n",
    "    \n",
    "    # Store the result as \"[line_number] [predicted_language]\"\n",
    "    results.append(f\"{idx} {predicted_language}\")\n",
    "\n",
    "# Write the classification results\n",
    "OUTPUT_FILE = \"../Data/Output/wordLangId2.out\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Output with Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 126 is different:\n",
      "Output: 126 Italian\n",
      "Solution: 126 French\n",
      "\n",
      "Total number of wrong predictions: 1\n",
      "Model Accuracy: 99.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "def compare_files(output_file, solution_file):\n",
    "    with open(output_file, 'r') as f1, open(solution_file, 'r') as f2:\n",
    "        output_lines = f1.readlines()\n",
    "        solution_lines = f2.readlines()\n",
    "    \n",
    "    # Ensure both files have the same number of lines\n",
    "    if (len(output_lines) != len(solution_lines)):\n",
    "        print(\"Files have different number of lines.\")\n",
    "\n",
    "    # Counter for the number of differences\n",
    "    diff_count = 0\n",
    "\n",
    "    for i in range(len(output_lines)):\n",
    "        if output_lines[i] != solution_lines[i]:\n",
    "            diff_count += 1\n",
    "            print(f\"Line {i + 1} is different:\")\n",
    "            print(f\"Output: {output_lines[i].strip()}\")\n",
    "            print(f\"Solution: {solution_lines[i].strip()}\")\n",
    "            print()\n",
    "\n",
    "    # Print the total number of differences\n",
    "    print(f\"Total number of wrong predictions: {diff_count}\")\n",
    "    print(f\"Model Accuracy: {(300-diff_count)/3}%\")\n",
    "    \n",
    "# File paths\n",
    "output_file = '../Data/Output/wordLangId2.out'\n",
    "solution_file = '../Data/Validation/labels.sol'\n",
    "\n",
    "# Call the function\n",
    "compare_files(output_file, solution_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
